{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aOUAb6ju6ICo"
   },
   "source": [
    "# Necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "vMXSH5itAPKO"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "from torch import nn\n",
    "from transformers import BertModel\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "moxMUbpt-aGp"
   },
   "outputs": [],
   "source": [
    "train_file = pd.read_json(\"../nyt_dataset/train_data.json\",encoding=\"utf-8\", orient='records')\n",
    "valid_file = pd.read_json(\"../nyt_dataset/valid_data.json\",encoding=\"utf-8\", orient='records')\n",
    "test_file = pd.read_json(\"../nyt_dataset/test_data.json\",encoding=\"utf-8\", orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens = [train_file['text'][i] for i in range(len(train_file['text']))]\n",
    "train_label = [train_file['relation_list'][i][0]['predicate'] for i in range(len(train_file['relation_list']))]\n",
    "valid_tokens = [valid_file['text'][i] for i in range(len(valid_file['text']))]\n",
    "valid_label = [valid_file['relation_list'][i][0]['predicate'] for i in range(len(valid_file['relation_list']))]\n",
    "test_tokens = [test_file['text'][i] for i in range(len(test_file['text']))]\n",
    "test_label = [test_file['relation_list'][i][0]['predicate'] for i in range(len(test_file['relation_list']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df = pd.DataFrame(); testing_df = pd.DataFrame(); validating_df = pd.DataFrame()\n",
    "training_df['Text'] = train_tokens; training_df['Ori_Label'] = train_label\n",
    "validating_df['Text'] = valid_tokens; validating_df['Ori_Label'] = valid_label\n",
    "testing_df['Text'] = test_tokens; testing_df['Ori_Label'] = test_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a list of all of the unique labels in the training and testing dataframes\n",
    "labellist=sorted(list(set(training_df['Ori_Label'].unique()).union(set(testing_df['Ori_Label'].unique()))))\n",
    "# Create a label dictionary\n",
    "labels={label:i for i,label in enumerate(labellist)}\n",
    "reverse_index={value:key for (key,value)in labels.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=BertTokenizer.from_pretrained('bert-base-cased')\n",
    "# Create a dataset class that inherits the properties of the Dataset class in Torch to pre-process and store the data\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,df,column='Text'):\n",
    "        # Convert label names to label indices using the `labels` dictionary \n",
    "        self.labels=[labels[label] for label in df['Ori_Label']]\n",
    "        # Tokenize the text data using the BERT tokenizer\n",
    "        self.texts=[tokenizer(text.lower(),padding='max_length',max_length=512,truncation=True,return_tensors=\"pt\") for text in df[column]]\n",
    "    def classes(self):\n",
    "        return self.labels\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    def get_batch_labels(self,idx):\n",
    "        return np.array(self.labels[idx])\n",
    "    def get_batch_texts(self,idx):\n",
    "        return self.texts[idx]\n",
    "    def __getitem__(self,idx):\n",
    "        batch_texts=self.get_batch_texts(idx)\n",
    "        batch_y=self.get_batch_labels(idx)\n",
    "        return batch_texts,batch_y\n",
    "# Create training and test datasets using the defined `Dataset` class\n",
    "train_data=Dataset(training_df)\n",
    "valid_data=Dataset(validating_df)\n",
    "test_data=Dataset(testing_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iUp1ui3YufIJ"
   },
   "source": [
    "# BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "nzj5CrSV12jj"
   },
   "outputs": [],
   "source": [
    "# Prepare inputs for the specific device (GPU or CPU) on which the model will run. Pre-check that GPU/CUDA is enabled\n",
    "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "wiwzj2mv2U1i"
   },
   "outputs": [],
   "source": [
    "def prepare_inputs(input1,label,device):\n",
    "  label=label.to(device)\n",
    "  mask=input1['attention_mask'].to(device)\n",
    "  input_id=input1['input_ids'].squeeze(1).to(device)\n",
    "  return (input_id,mask,label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "v5ur2F804Jhm"
   },
   "outputs": [],
   "source": [
    "# Create a classification layer on top of BERT\n",
    "class BertClassifier(nn.Module):\n",
    "    # Define Bert model, Dropout layer, linear layer and activation function\n",
    "    def __init__(self,dropout=0.5,num_classes=2):\n",
    "        super(BertClassifier,self).__init__()\n",
    "        self.bert=BertModel.from_pretrained('bert-base-cased')\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "        self.linear=nn.Linear(768,num_classes)\n",
    "        self.relu=nn.ReLU()\n",
    "    # Define the computational flow of the model    \n",
    "    def forward(self,input_id,mask):\n",
    "        last_hidden_layer,pooled_output = self.bert(input_ids=input_id,attention_mask=mask,return_dict=False)\n",
    "        dropout_output=self.dropout(pooled_output)\n",
    "        linear_output=self.linear(dropout_output)\n",
    "        final_layer=self.relu(linear_output)\n",
    "        return final_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "ca3ulp9r4dXm"
   },
   "outputs": [],
   "source": [
    "# Use the definitions of the initialisation method and the forward method of the BertClassifier class to sketch out what the neural network architecture looks like.\n",
    "def train(model, train_data,val_data,learning_rate,epochs,batchSize):\n",
    "    train_dataloader=torch.utils.data.DataLoader(train_data,batch_size=batchSize,shuffle=True)\n",
    "    val_dataloader=torch.utils.data.DataLoader(val_data,batch_size=batchSize)\n",
    "    # Determine if cuda can be called, and call cpu if cuda cannot be called\n",
    "    use_cuda=torch.cuda.is_available()\n",
    "    device=torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    # Define loss function, optimizer and learning rate\n",
    "    criterion=nn.CrossEntropyLoss()\n",
    "    optimizer=Adam(model.parameters(),lr=learning_rate)\n",
    "    if use_cuda:\n",
    "        model=model.cuda()\n",
    "        criterion=criterion.cuda()\n",
    "        \n",
    "    for epoch_num in range(epochs):\n",
    "        # Initialize the cumulative training set accuracy and training set loss\n",
    "        total_acc_train=0\n",
    "        total_loss_train=0\n",
    "        # Set the model to training mode\n",
    "        model.train()\n",
    "        for train_input,train_label in tqdm(train_dataloader):\n",
    "            input_id,mask, train_label=prepare_inputs(train_input,train_label,device)\n",
    "            output=model(input_id,mask)\n",
    "            # Pass the input data to the model for forward propagation, calculate the loss, and accumulate the training set accuracy and training set loss\n",
    "            batch_loss=criterion(output,train_label.long())\n",
    "            total_loss_train +=batch_loss.item()\n",
    "            acc=(output.argmax(dim=1)==train_label).sum().item()\n",
    "            total_acc_train+=acc\n",
    "            # Clear the calculated gradient information\n",
    "            model.zero_grad()\n",
    "            # Calculate the gradient of the loss function with respect to the model parameters\n",
    "            batch_loss.backward()\n",
    "            # Update the parameters in the model to make it more optimal towards the training goal\n",
    "            optimizer.step()\n",
    "        # Initialize the cumulative validation set accuracy and validation set loss    \n",
    "        total_acc_val=0\n",
    "        total_loss_val=0\n",
    "        # Set the model to evaluation mode\n",
    "        model.eval()\n",
    "        # Close gradient calculation\n",
    "        with torch.no_grad():\n",
    "            # Iterate through all the data in the validation set to evaluate the performance of the model\n",
    "            for val_input,val_label in val_dataloader:\n",
    "                input_id,mask, val_label=prepare_inputs(val_input,val_label,device)\n",
    "                output=model(input_id,mask)\n",
    "                # Calculate and accrue losses\n",
    "                batch_loss=criterion(output,val_label.long())\n",
    "                total_loss_val+=batch_loss.item()\n",
    "                # Cumulative number of correctly predicted samples\n",
    "                acc=(output.argmax(dim=1)==val_label).sum().item()\n",
    "                total_acc_val+=acc\n",
    "        print(f'Epochs: {epoch_num+1} | Train Loss: {total_loss_train / len(train_data):.3f} | Train Accuracy: {total_acc_train/len(train_data):.3f}')\n",
    "        print(f'Val loss: {total_loss_val/len(val_data):.3f} | Val Accuracy: {total_acc_val / len(val_data):.3f}')\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yVK_laRY4eRa",
    "outputId": "413ffa1d-c85b-4aa7-bf2e-5cf2dcaac47e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Define the number of epochs, the learning rate and an instance of BertClassifier network.\n",
    "model=BertClassifier(num_classes=len(labels.keys()))\n",
    "model=model.to(device)\n",
    "EPOCHS=6\n",
    "LR=1e-5\n",
    "batch_size=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QEn9xgKz4fX9",
    "outputId": "0ea45568-2179-4cc6-f0d5-fe660841a8a6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28098/28098 [1:38:56<00:00,  4.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 1 | Train Loss: 0.431 | Train Accuracy: 0.723\n",
      "Val loss: 0.323 | Val Accuracy: 0.750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28098/28098 [1:37:53<00:00,  4.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 2 | Train Loss: 0.281 | Train Accuracy: 0.780\n",
      "Val loss: 0.301 | Val Accuracy: 0.772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28098/28098 [1:36:09<00:00,  4.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 3 | Train Loss: 0.221 | Train Accuracy: 0.814\n",
      "Val loss: 0.288 | Val Accuracy: 0.781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28098/28098 [1:35:05<00:00,  4.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 4 | Train Loss: 0.184 | Train Accuracy: 0.837\n",
      "Val loss: 0.304 | Val Accuracy: 0.776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28098/28098 [1:35:17<00:00,  4.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 5 | Train Loss: 0.158 | Train Accuracy: 0.857\n",
      "Val loss: 0.311 | Val Accuracy: 0.780\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28098/28098 [1:34:42<00:00,  4.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 6 | Train Loss: 0.136 | Train Accuracy: 0.878\n",
      "Val loss: 0.351 | Val Accuracy: 0.772\n"
     ]
    }
   ],
   "source": [
    "# Train the BERT model\n",
    "train(model,train_data,valid_data,LR,EPOCHS,batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "F9v1d2xh4jCk"
   },
   "outputs": [],
   "source": [
    "# Save model\n",
    "output_dir = \"bert-base-cased-textM-E\" + str(EPOCHS)\n",
    "torch.save(model,output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "b1CCU5614lBY"
   },
   "outputs": [],
   "source": [
    "# Loda model\n",
    "class BertClassifier(nn.Module):\n",
    "    # Define Bert model, Dropout layer, linear layer and activation function\n",
    "    def __init__(self,dropout=0.5,num_classes=2):\n",
    "        super(BertClassifier,self).__init__()\n",
    "        self.bert=BertModel.from_pretrained('bert-base-cased')\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "        self.linear=nn.Linear(768,num_classes)\n",
    "        self.relu=nn.ReLU()\n",
    "    # Define the computational flow of the model    \n",
    "    def forward(self,input_id,mask):\n",
    "        last_hidden_layer,pooled_output = self.bert(input_ids=input_id,attention_mask=mask,return_dict=False)\n",
    "        dropout_output=self.dropout(pooled_output)\n",
    "        linear_output=self.linear(dropout_output)\n",
    "        final_layer=self.relu(linear_output)\n",
    "        return final_layer\n",
    "\n",
    "EPOCHS = 6\n",
    "input_dir = \"bert-base-cased-textM-E\" + str(EPOCHS)\n",
    "complete_model = torch.load(input_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "Z_IVs3Sa4lt_"
   },
   "outputs": [],
   "source": [
    "batchsize=2\n",
    "# Model evaluation function\n",
    "def evaluate(model,test_dataset):\n",
    "    model.eval()\n",
    "    test_dataloader=torch.utils.data.DataLoader(test_dataset,batch_size=batchsize)\n",
    "    \n",
    "    # Determine if cuda can be called, and call cpu if cuda cannot be called\n",
    "    use_cuda=torch.cuda.is_available()\n",
    "    device=torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    \n",
    "    if use_cuda:\n",
    "        model=model.cuda()\n",
    "        \n",
    "    total_acc_test=0\n",
    "    with torch.no_grad():\n",
    "        count=0\n",
    "        predictions=[]\n",
    "        for test_input,test_label in tqdm(test_dataloader):\n",
    "            count+=batchsize\n",
    "            test_label=test_label.to(device)\n",
    "            mask=test_input['attention_mask'].to(device)\n",
    "            input_id=test_input['input_ids'].squeeze(1).to(device)\n",
    "            output=model(input_id,mask)\n",
    "            # save the prediction for further analysis\n",
    "            predictions.append(output.argmax(dim=1))  \n",
    "            acc=(output.argmax(dim=1)==test_label).sum().item()\n",
    "            \n",
    "            total_acc_test+=acc\n",
    "            \n",
    "    # Show label prediction accuracy        \n",
    "    print(f'Test accuracy: {total_acc_test/len(test_dataset): .3f}')\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3ws1hvfr4nPz",
    "outputId": "e35832e0-899f-4c25-b42e-cf322fce9893"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2500/2500 [03:39<00:00, 11.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:  0.757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Test set label prediction\n",
    "predictions = evaluate(complete_model, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "GcItP5WfHxC7"
   },
   "outputs": [],
   "source": [
    "# Add the predicted label for each test item to the dataframe with the test data\n",
    "flattened=[]\n",
    "for batch in predictions:\n",
    "    for pred in batch:\n",
    "        flattened.append(reverse_index[pred.item()])\n",
    "testing_df['Predict_Label']=flattened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_pre_label = testing_df['Predict_Label']\n",
    "result_ori_label = testing_df['Ori_Label']\n",
    "resule_labels = list(set(list(result_ori_label)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Predict Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "JB13xRg0YNl8"
   },
   "outputs": [],
   "source": [
    "# Save the dataframe with test data and predicted label\n",
    "test_dir = \"BERT_result_TextM_E\" + str(EPOCHS) + \".csv\"\n",
    "testing_df.to_csv(test_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eWfxAReLwbFi"
   },
   "source": [
    "# Load Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "YiYRwLBJwdh9"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "  test_dir = \"BERT_result_TextM_E\" + str(EPOCHS) + \".csv\"\n",
    "  result_file_dir = test_dir\n",
    "  result_file = pd.read_csv(result_file_dir)\n",
    "  result_pre_label = result_file['Predict_Label']\n",
    "  result_ori_label = result_file['Ori_Label']\n",
    "  resule_labels = list(set(list(result_ori_label)))\n",
    "except:\n",
    "  print('Select File Wrong!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "5N1xMfZfwhqu"
   },
   "outputs": [],
   "source": [
    "tp={}; fp={}; fn={}; tn={}\n",
    "# Iteration statistics prediction results\n",
    "for label1,pred1 in zip(result_ori_label,result_pre_label):\n",
    "  for label in resule_labels:\n",
    "    if label1==label:\n",
    "      if pred1==label:\n",
    "        tp[label]=tp.get(label,0)+1\n",
    "      else:\n",
    "        fn[label]=fn.get(label,0)+1\n",
    "    else:\n",
    "      if pred1==label:\n",
    "        fp[label]=fp.get(label,0)+1\n",
    "      else:\n",
    "        tn[label]=tn.get(label,0)+1\n",
    "# Computational performance evaluation metrics\n",
    "precision = {label:value/(value+fp.get(label,0)) for label,value in tp.items()}\n",
    "recall = {label:value/(value+fn.get(label,0)) for label,value in tp.items()}\n",
    "f1 = {label:(2*value*recall.get(label,0))/(value+recall.get(label,0)) for label,value in precision.items()}\n",
    "accuracy = {'test2_accuracy':sum(tp.values())/len(result_pre_label) for label,value in precision.items()}\n",
    "f1_weighted = sum([value*len(result_file[result_file['Ori_Label']==label])/len(result_file) for label,value in f1.items()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "weCXVEOzfNwG"
   },
   "source": [
    "## Evaluation Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o99U63OFacxO",
    "outputId": "398885a9-07a5-4707-d605-b3d8abc52a9e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test2_accuracy': 0.7574}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WK2bx6CHfRun",
    "outputId": "b156c474-1056-49e9-9069-bcc68582e268"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'/people/person/children': 0.8055555555555556,\n",
       " '/location/location/contains': 0.869155206286837,\n",
       " '/business/person/company': 0.7567567567567568,\n",
       " '/people/person/nationality': 0.8507462686567164,\n",
       " '/people/person/place_lived': 0.7762376237623763,\n",
       " '/people/person/place_of_birth': 0.5849056603773585,\n",
       " '/location/neighborhood/neighborhood_of': 0.48120300751879697,\n",
       " '/location/country/capital': 0.5343137254901961,\n",
       " '/location/country/administrative_divisions': 0.27715355805243447,\n",
       " '/people/deceased_person/place_of_death': 0.3333333333333333,\n",
       " '/business/company/founders': 0.5,\n",
       " '/business/company/place_founded': 0.4857142857142857,\n",
       " '/location/administrative_division/country': 0.3170731707317073,\n",
       " '/sports/sports_team_location/teams': 0.6666666666666666,\n",
       " '/business/company/advisors': 1.0,\n",
       " '/business/company_shareholder/major_shareholder_of': 0.43478260869565216,\n",
       " '/people/person/religion': 0.5714285714285714}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JKjR5H7vfR3U",
    "outputId": "7dde54ec-c58b-4a58-f728-eb39678d354a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'/people/person/children': 0.9354838709677419,\n",
       " '/location/location/contains': 0.8540540540540541,\n",
       " '/business/person/company': 0.9333333333333333,\n",
       " '/people/person/nationality': 0.8807947019867549,\n",
       " '/people/person/place_lived': 0.795131845841785,\n",
       " '/people/person/place_of_birth': 0.3875,\n",
       " '/location/neighborhood/neighborhood_of': 0.3764705882352941,\n",
       " '/location/country/capital': 0.40671641791044777,\n",
       " '/location/country/administrative_divisions': 0.5211267605633803,\n",
       " '/people/deceased_person/place_of_death': 0.5294117647058824,\n",
       " '/business/company/founders': 0.3793103448275862,\n",
       " '/business/company/place_founded': 0.5862068965517241,\n",
       " '/location/administrative_division/country': 0.16666666666666666,\n",
       " '/sports/sports_team_location/teams': 0.3076923076923077,\n",
       " '/business/company/advisors': 0.6666666666666666,\n",
       " '/business/company_shareholder/major_shareholder_of': 0.7692307692307693,\n",
       " '/people/person/religion': 0.8}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "maXjENWOfSCg",
    "outputId": "bba8b6ce-290f-48e6-87c0-e917597cf6c0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'/people/person/children': 0.8656716417910448,\n",
       " '/location/location/contains': 0.8615384615384616,\n",
       " '/business/person/company': 0.835820895522388,\n",
       " '/people/person/nationality': 0.8655097613882864,\n",
       " '/people/person/place_lived': 0.7855711422845693,\n",
       " '/people/person/place_of_birth': 0.46616541353383456,\n",
       " '/location/neighborhood/neighborhood_of': 0.4224422442244224,\n",
       " '/location/country/capital': 0.461864406779661,\n",
       " '/location/country/administrative_divisions': 0.36185819070904646,\n",
       " '/people/deceased_person/place_of_death': 0.409090909090909,\n",
       " '/business/company/founders': 0.4313725490196078,\n",
       " '/business/company/place_founded': 0.53125,\n",
       " '/location/administrative_division/country': 0.21848739495798317,\n",
       " '/sports/sports_team_location/teams': 0.42105263157894735,\n",
       " '/business/company/advisors': 0.8,\n",
       " '/business/company_shareholder/major_shareholder_of': 0.5555555555555555,\n",
       " '/people/person/religion': 0.6666666666666666}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7536582269247721"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
